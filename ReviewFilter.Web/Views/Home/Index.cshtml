@model ReviewFilter.Web.Models.HomeViewModel

@{
    ViewBag.Title = "Content Verification";
}

<h1>AI for Fake Review Detection</h1>

<form asp-action="VerifyContent" method="post">
    <div class="review-container">
        <textarea id="ReviewContent" name="ReviewContent" class="review-input" rows="4" placeholder="Write a review.">@Model.ReviewContent</textarea>

        <button type="submit" class="send-button">Submit the review</button>
    </div>
</form>

@if (Model.VerificationResult != null)
{
    <div class="mt-3">
        <h2>Detection results</h2>
        <div class="alert @(Model.VerificationResult.Success ? "alert-success" : "alert-danger")">
            @if (Model.VerificationResult.Success)
            {
                <p>Possible sexual content: @Model.VerificationResult.SexualContent</p>
                <p>Possible hate content: @Model.VerificationResult.HateContent</p>
                <p>Possible harassment content: @Model.VerificationResult.HarassmentContent</p>
                <p>Sentiment: @Model.VerificationResult.Sentiment</p>
                <hr/>
                <p>ML result: @Model.MLResult [CG (Fake- computer generated) and  OG (valid)]</p>
                <p>Exaggerated word count: @Model.ExaggeratedWordsCount</p>
@*                 <p>Similarity result:  @($"{Model.SimilarityResult * 100:F2}%")</p> *@
                <p>Similar review count: @Model.SimilarReviewsCount (AI api limited for 3 request per minute)</p>
                <h3>Conclusion:</h3>
                <p>@Model.Conclusion</p>
            }
            else
            {
                <p>Verification failed!</p>
                <p>Error: @Model.VerificationResult.ErrorMessage</p>
            }
        </div>
    </div>

    <div class="conclusion-description-container">
        <h4>Conclusion calculation description:</h4>

        <p>This function is used to analyze the results of a review verification process and determine if the review is likely to be fake. The analysis is based on various factors such as sentiment, machine learning results, exaggerated words, similar reviews, and content flags for inappropriate content. The function calculates a "fake score" based on these factors, and provides a conclusion about the authenticity of the review.</p>

        <h5>Key Steps in the Function:</h5>
        <ol>
            <li><strong>Initialize the Fake Score:</strong> The function starts by initializing a <code>fakeScore</code> variable to 0. This will accumulate weights based on the different factors related to fake review detection.</li>

            <li>
                <strong>Fabricated Positive Review:</strong>
                <p>If the sentiment of the review is positive, and one or more of the following conditions are met:</p>
                <ul>
                    <li>The machine learning result indicates it is computer-generated (<code>MLResult == "CG"</code>),</li>
                    <li>The review contains more than 2 exaggerated words,</li>
                    <li>The review has more than 3 similar reviews,</li>
                </ul>
                <p>Then, <code>0.2</code> is added to the <code>fakeScore</code>.</p>
            </li>

            <li>
                <strong>Fabricated Negative Review:</strong>
                <p>If the sentiment of the review is negative, and the same conditions apply (as above for positive reviews), another <code>0.2</code> is added to the <code>fakeScore</code>.</p>
            </li>

            <li>
                <strong>ML Result Contribution:</strong>
                <p>If the machine learning result indicates that the review is computer-generated (<code>MLResult == "CG"</code>), an additional <code>0.4</code> is added to the <code>fakeScore</code>.</p>
            </li>

            <li>
                <strong>Content Flags:</strong>
                <p>The function checks if the review contains inappropriate content based on three flags: sexual content, hate content, and harassment content. If any of these exceed a threshold of <code>0.01</code>, <code>0.1</code> is added to the <code>fakeScore</code> for each flag triggered.</p>
            </li>

            <li>
                <strong>Final Decision:</strong>
                <p>After calculating the <code>fakeScore</code>, the function compares it to a <code>fakeThreshold</code> of <code>0.5</code>.</p>
                <ul>
                    <li>If the <code>fakeScore</code> is greater than <code>0.5</code>, the review is deemed likely to be fake, and the function returns a message stating that the review is "likely to be FAKE" with a calculated percentage based on the fake score.</li>
                    <li>If the <code>fakeScore</code> is less than or equal to <code>0.5</code>, the review is considered valid, and the function returns a message indicating that the review is "VALID" with a corresponding percentage.</li>
                </ul>
            </li>
        </ol>

        <h5>Returned Value:</h5>
        <p>The function returns a string message that indicates whether the review is likely fake or valid, along with a percentage that reflects the likelihood of the conclusion.</p>
    </div>
}

<div class="criteria-container">
    <p style='margin-top:0in;margin-right:0in;margin-bottom:8.0pt;margin-left:0in;line-height:115%;font-size:16px;font-family:"Aptos",sans-serif;'><strong>Jury Criteria:</strong>&nbsp;</p>
    <ul style="list-style-type: disc;">
        <li>How effectively did the solution identify fake reviews?</li>
        <li>Is there a complete picture of the end goal of the tool? (e.g: we want to focus on these websites, or on these kinds of websites, for these languages, etc.)</li>
        <li>Is there a clear idea of a data flow from ingestion (e.g: website scrapping) to result serving?</li>
        <li>Is the processing time reasonable or is there an idea how to make it reasonable?</li>
        <li>Result serving - what have they decided to include, what have they decided not to include and more importantly, why? Are the results visually confusing? Are they misleading?</li>
        <li>&nbsp;What do they consider when determining the quality of a product? That is, how do they define a fake review? For the reviews that are deemed valid - how do they define a good review, what about a bad review? Do they go behind the reviews into the domain of the users generating the reviews - if, yes, how and why? If no, why?</li>
        <li>Are there ideas that they did not have time to implement and are still a work in progress? What are those? (in the form of next steps if this were their start-up product)</li>
        <li>ML specific process: pre-processing + data sources, training/finetuning + retraining (plans and drift), validation, model serving.</li>
    </ul>
</div>